\documentclass[12pt]{article}
\usepackage{myart}\addbibresource{/Users/lwg342/Documents/LaTeX/lib.bib}
%--------Hello World--------%
%=====================================%
\begin{document}
    \title{Prediction with Interval Outcomes}

    \section{Procedure Summary}

    \blue{Blue items are parameters of simulation designs that can be changed.}

    We randomly generate \blue{sample size \(N\)} observations of \(\cqty{(y_{i},x_{i}), i = 1 ,\dots, N}\), where \(x\) is \blue{\(K\)-dimensional}, and
    \begin{equation*}
        y_{i} = \blue{f}(x_{i}) + \epsilon_{i}.
    \end{equation*}
    where we will consider the following designs, 
    \begin{align*}
        x_{i} &\sim \begin{cases}
            N(0,I_{K}) \qq{i.i.d standard normal}\\
            U( - \sqrt{3},\sqrt{3}) \qq{i.i.d uniform for each \(x_{ik}, k = 1,\dots, K\)}
        \end{cases} \\
        \epsilon &\sim \begin{cases}
            N(0,\blue{v_{\epsilon}}) \qq{i.i.d normal}\\
            \frac{\chi^{2}_{\text{df}} - \text{df}}{\sqrt{2 \text{df}}} \qq{i.i.d centered \(\chi^{2}\) with \blue{df} degrees of freedom}
        \end{cases}
    \end{align*}
    and \(\blue{f}\) is a function we will specify. 
    Let's assume we observe not \(y_{i}\), but \(y^{l}_{i}\) and \(y_{i}^{u}\). In the simulation, it's generated by 
    \begin{align*}
        y_{i}^{l} &= \lfloor y_{i}/s \rfloor \times s - b_{1} \\
        y_{i}^{u} &= \pqty{\lfloor y_{i}/s \rfloor + 1} \times s  + b_{2},
    \end{align*}
    where the \blue{scale} \(s\) will control the scale/length of the interval observations and \blue{\(b_{1},b_{2}\)} will control the bias of the interval observation, in what follows let's first take \(b_{1} = b_{2} = 0\). We follow Elie's suggestions and do the following:
    \begin{enumerate}
        \item Split the sample into training and testing sets of equal size, \(\mathcal{I}_{\text{train}}, \mathcal{I}_{\text{test}}\). 
        \item For \(m = 1,\dots, \blue{M}\), randomly pick \(\lambda_{m} \sim \blue{\text{Beta}(p_{1}, p_{2})}\), and construct random draws of observations from the interval \(y_{m,i}^{d} = \lambda_{m}y^{l}_{i} + (1 - \lambda_{m}) y^{u}_{i}\). In this way, we we have constructed \(M\) training data sets \(\mathcal{D}_{m} = \cqty{(x_{i}, y_{m,i}^{d}): i \in \mathcal{I}_{\text{train}}}\), \(m\leq M\). 
        \item For each \(\mathcal{D}_{m}\), fit a \blue{regression model} \(\hat{f}_{m}\) with a method of choice. Let \(\tilde{Y}(x) = \left\{\hat{f}_{m}(x): 1\leq m\leq M\right\}\) be the \textit{pre-selection prediction sets} for a given evaluation point \(x\), which is the set of predictions from all trained regressions.
        \item Compute the loss \(L_{m}\) for each trained regression model on the testing set,
        %     diff_l = np.maximum(yl_test - y_test_prediction, 0) ** 2
        % diff_u = np.maximum(y_test_prediction - yu_test, 0) ** 2
        % return (diff_l + diff_u).mean(axis=1)
        \begin{equation*}
            L_{m} = \frac{1}{\abs{\mathcal{I}_{\text{test}}}}\sum_{i = \mathcal{I}_{\text{test}}} \pqty{\max\cqty{y_{i}^{l} - \hat{f}_{m}(x_{i}), 0}^{2} + \max\cqty{\hat{f}_{m}(x_{i}) - y_{i}^{u}, 0}^{2}}.
        \end{equation*}
        and then find the  minimum \(L^{*} = \min_{m} L_{m}\) and \(\mathcal{M}^{*} = \cqty{m: L_{m} \leq L^{*} + \tau}\), where \blue{\(\tau\)} is the \blue{tolerance}. And the \textit{post-selection prediction sets} are \(\hat{Y}(x) = \cqty{\hat{f}_{m}(x): m\in \mathcal{M}^{*}}\).
    \end{enumerate}

    \section{Simulations about the methods of regression}
        Let's consider a nonlinear function with \(K = 4\),
        \begin{equation*}
            f(x) = \sqrt{2} + \sum_{k = 1}^{K} \beta_{k} x_{k} + \cos(3x_{K}) + 0.5 x_{K}^{2}.
        \end{equation*}
        Here, only the last covariate enters nonlinearly. In the plots, ``fit y\_true'' and ``fit y\_middle'' are the predictions we make if we train the regression model on the true \(y\) and the middle of the interval \(\frac{y^{l} + y^{u}}{2}\) respectively.
        For visualization we will fix the first three covariates to be zero and vary the last covariate.

        % figure 1: simulation-results/202312071143-loclin-1000-200-4.pdf
        \begin{figure}[htbp]
            \centering
            \includegraphics[width=0.9\linewidth]{../simulation-results/202312071143-loclin-1000-200-4.pdf}
            \caption{Local Linear Regression}
            \label{fig:loclin}
        \end{figure}
        % figure 2: subfig1 : simulation-results/202312071203-rf-1000-200-4.pdf; subfig2 : simulation-results/202312071205-rf-1000-200-4.pdf
        \begin{figure}[ht]
            \centering
            \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{../simulation-results/202312071203-rf-1000-200-4.pdf}
                \caption{Max depth = 5}
                \label{fig:1}
            \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{../simulation-results/202312071205-rf-1000-200-4.pdf}
                \caption{Max depth = 10}
                \label{fig:2}
            \end{subfigure}
            \caption{Random Forest with different max depths, number of trees is 200.}
            \label{fig:comparison1}
        \end{figure}

        % figure 3: subfig1 : simulation-results/202312071213-krr-1000-200-4.pdf; subfig2 : simulation-results/202312071214-krr-1000-200-4.pdf

        \begin{figure}[ht]
            \centering
            \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{../simulation-results/202312071213-krr-1000-200-4.pdf}
                \caption{Regularization strength = 0.5}
                \label{fig:krr-1}
            \end{subfigure}
            \hfill
            \begin{subfigure}[b]{0.45\textwidth}
                \includegraphics[width=\textwidth]{../simulation-results/202312071214-krr-1000-200-4.pdf}
                \caption{Regularization strength = 0.1}
                \label{fig:krr-2}
            \end{subfigure}
            \caption{Kernel Ridge Regression with different regularization strengths. The kernel is RBF.}
            \label{fig:comparison2}
        \end{figure}
        

        \newpage

    \section{Simulations about the random draws \tmath{y^{d}_{m}}}

    There are two considerations about the ways we draw \(y^{d}_{m}\), the number of draws and distribution of the weights. In principle, we can let \(M\) be as large as we want. But in the simulations, it seems to be not very important. On the other hand, the scale of the interval observation and the distribution of the weights are important. 

    \subsection{Comparing the distributional distance}
    Table \ref{tab:wasserstein-combined} shows the Wasserstein distances between the empirical distribution of \(y\) where the scale of interval \(s = 1.0\) and the \(y^{d}_{m}\) we draw by uniformly drawing \(\lambda_{m,i}\sim \text{Beta}(1,1) = U(0,1)\). As \(M\) increases, the minimum Wasserstein distance decreases, in the sense that at least one random draws \(y^{d}_{m}\) mimics the empirical distribution of \(y\). But overall, all draws are close in distribution to \(y\).

    We also test the effects of \(M\) on the conditional distribution by fixing \(x = 0\) and randomly generate \(y, y^{l}, y^{u}\) according to the same DGP. Then we compare the empirical distribution of \(y\) and the distribution of \(y^{d}_{m}\) in Table \ref{tab:wasserstein-combined}.  

    \begin{table}[htbp]
        \centering
        \begin{tabular}{ccccccccc}
            \hline
            & \multicolumn{2}{c}{Unconditional} & & \multicolumn{2}{c}{Conditional} \\
            \cline{2-3} \cline{5-6}
            \(M\) & Avg. Dist. & Min. Dist. & & Avg. Dist. & Min. Dist. \\
            \hline
            10 & 0.062 & 0.050 & & 0.066 & 0.057 \\
            200 & 0.062 & 0.043 & & 0.069 & 0.047 \\
            2000 & 0.062 & 0.039 & & 0.069 & 0.044 \\
            10000 & 0.062 & 0.037 & & 0.069 & 0.045 \\
            \hline
        \end{tabular}
        \caption{Wasserstein Distances for Different \(M\) Values (Unconditional vs Conditional)}
        \label{tab:wasserstein-combined}
    \end{table}

    On the other hand, the scale of the interval is important. Table \ref{tab:wasserstein-scale} shows the Wasserstein distances between the empirical distribution of \(y\) and the \(y^{d}_{m}\) when we fix \(M = 2000\), 
    \begin{table}[htbp]
        \centering
        \begin{tabular}{ccccccccc}
            \hline
            & \multicolumn{2}{c}{Unconditional} & & \multicolumn{2}{c}{Conditional} \\
            \cline{2-3} \cline{5-6}
            \(s\) & Avg. Dist. & Min. Dist. & & Avg. Dist. & Min. Dist. \\
            \hline
            1 & 0.055 & 0.031 & & 0.069 & 0.043 \\
            2 & 0.17 & 0.12 & & 0.26& 0.20 \\
            3 & 0.28& 0.22 & & 0.59 & 0.50 \\
            4 & 0.37 & 0.30 & & 0.42 & 0.34 \\
            \hline
        \end{tabular}
        \caption{Wasserstein Distances when the scale of the interval \(s\) varies.}
        \label{tab:wasserstein-scale}
    \end{table}
    
    And the distribution of the weights seems to play a lesser role than the scale. Table \ref{tab:wasserstein-Beta} shows the Wasserstein distances between the empirical distribution of \(y\) and the \(y^{d}_{m}\) when we fix \(M = 2000\) and \(s = 2.0\), and draw weights from \(\text{Beta}(p_{1},p_{2})\). 

    \begin{table}[htbp]
        \centering
        \begin{tabular}{ccccccccc}
            \hline
            && \multicolumn{2}{c}{Unconditional} & & \multicolumn{2}{c}{Conditional} \\
            \cline{3-4} \cline{6-7}
            \(p_{1}\) & \(p_{2}\) & Avg. Dist. & Min. Dist. & & Avg. Dist. & Min. Dist. \\
            \hline
            0.5 & 0.5 & 0.093 & 0.072 & & 0.104 & 0.081 \\
            0.5 & 1 & 0.166 & 0.131 & & 0.182 & 0.155 \\
            0.5 & 2 & 0.300 & 0.274 & & 0.292 & 0.269 \\
            1 & 0.5 & 0.166 & 0.137 & & 0.178 & 0.149 \\
            1 & 1 & 0.056 & 0.035 & & 0.072 & 0.047 \\
            1 & 2 & 0.170 & 0.147 & & 0.157 & 0.129 \\
            2 & 0.5 & 0.302 & 0.276 & & 0.305 & 0.283 \\
            2 & 1 & 0.157 & 0.131 & & 0.167 & 0.137 \\
            2 & 2 & 0.078 & 0.064 & & 0.083 & 0.066 \\
            \hline
        \end{tabular}
        \caption{Wasserstein Distances for Different \(p_{1}\) and \(p_{2}\) Values (Unconditional vs Conditional)}
        \label{tab:wasserstein-Beta}
    \end{table}
    \newpage
    \subsection{Visualization of the distribution}
    It seems that how we draw \(\lambda\) will affect how we can recover the distribution of \(y\). Let's consider the conditional case where we fix \(x = 0\).  

    % \includegraphics{../simulation-results/202312071609-conditional-1-1.pdf}
    % \includegraphics{../simulation-results/202312071609-conditional-2-2.pdf}
    % \includegraphics{../simulation-results/202312071609-conditional-0.5-0.5.pdf}
    % \includegraphics{../simulation-results/202312071609-conditional-0.5-2.pdf}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{../simulation-results/202312071609-conditional-1-1.pdf}
        \caption{Conditional on \(x = 0\), \(p_{1} = 1\), \(p_{2} = 1\)}
        \label{fig:sub1}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{../simulation-results/202312071609-conditional-2-2.pdf}
        \caption{Conditional on \(x = 0\), \(p_{1} = 2\), \(p_{2} = 2\)}
        \label{fig:sub2}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{../simulation-results/202312071610-conditional-0.5-0.5.pdf}
        \caption{Conditional on \(x = 0\), \(p_{1} = 0.5\), \(p_{2} = 0.5\)}
        \label{fig:sub3}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{../simulation-results/202312071610-conditional-0.5-2.pdf}
        \caption{Conditional on \(x = 0\), \(p_{1} = 0.5\), \(p_{2} = 2\)}
        \label{fig:sub4}
    \end{subfigure}
    \caption{Conditional simulations}
    \label{fig:test}
\end{figure}

From the figure we can see that uniform draws do a good job. But to get better fit, in practice, it may be worth considering
\begin{enumerate}
    \item Random \(p_{1}, p_{2}\) values;
    \item \(p_{1},p_{2}\) values that are specific to the interval;
    \item Ensure some kind of ``smoothness'' in the density of the draws.
\end{enumerate}

\newpage
\subsection{In terms of prediction} 


\end{document}
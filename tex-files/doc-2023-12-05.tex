\documentclass[11pt]{article}
\usepackage{myart}\addbibresource{/Users/lwg342/Documents/LaTeX/lib.bib}
%--------Hello World--------%
%=====================================%
\begin{document}
    \title{Prediction with Interval Outcomes}

    \section{Procedure Summary}

    \blue{Blue items are parameters of simulation designs that can be changed.}

    We randomly generate \blue{sample size \(N\)} observations of \(\cqty{(y_{i},x_{i}), i = 1 ,\dots, N}\), where \(x\) is \blue{\(K\)-dimensional}, and
    \begin{equation*}
        y_{i} = \blue{f}(x_{i}) + \epsilon_{i}.
    \end{equation*}
    where we will consider the following designs, 
    \begin{align*}
        x_{i} &\sim \begin{cases}
            N(0,I_{K}) \qq{i.i.d standard normal}\\
            U( - \sqrt{3},\sqrt{3}) \qq{i.i.d uniform for each \(x_{ik}, k = 1,\dots, K\)}
        \end{cases} \\
        \epsilon &\sim \begin{cases}
            N(0,\blue{v_{\epsilon}}) \qq{i.i.d normal}\\
            \frac{\chi^{2}_{\text{df}} - \text{df}}{\sqrt{2 \text{df}}} \qq{i.i.d centered \(\chi^{2}\) with \blue{df} degrees of freedom}
        \end{cases}
    \end{align*}
    and \(\blue{f}\) is a function we will specify. 
    Let's assume we observe not \(y_{i}\), but \(y^{l}_{i}\) and \(y_{i}^{u}\). In the simulation, it's generated by 
    \begin{align*}
        y_{i}^{l} &= \lfloor y_{i}/b \rfloor \times b - a_{1} \\
        y_{i}^{u} &= \pqty{\lfloor y_{i}/b \rfloor + 1} \times b  + a_{2},
    \end{align*}
    where \blue{b} will control the scale/length of the interval observations and \blue{\(a_{1},a_{2}\)} will control the bias, in what follows let's first take \(a_{1} = a_{2} = 0\). We follow Elie's suggestions and do the following:
    \begin{enumerate}
        \item Split the sample into training and testing sets of equal size, \(\mathcal{I}_{\text{train}}, \mathcal{I}_{\text{test}}\). 
        \item For \(m = 1,\dots, \blue{M}\), randomly pick \(\lambda_{m} \sim \blue{\text{Beta}(p_{1}, p_{2})}\), and construct random draws of observations from the interval \(y_{m,i}^{d} = \lambda_{m}y^{l}_{i} + (1 - \lambda_{m}) y^{u}_{i}\). In this way, we we have constructed \(M\) training data sets \(\mathcal{D}_{m} = \cqty{(x_{i}, y_{m,i}^{d}): i \in \mathcal{I}_{\text{train}}}\), \(m\leq M\). 
        \item For each \(\mathcal{D}_{m}\), fit a \blue{regression model} \(\hat{f}_{m}\) with a method of choice. Let \(\tilde{Y}(x) = \left\{\hat{f}_{m}(x): 1\leq m\leq M\right\}\) be the \textit{pre-selection prediction sets} for a given evaluation point \(x\), which is the set of predictions from all trained regressions.
        \item Compute the loss \(L_{m}\) for each trained regression model on the testing set,
        %     diff_l = np.maximum(yl_test - y_test_prediction, 0) ** 2
    % diff_u = np.maximum(y_test_prediction - yu_test, 0) ** 2
    % return (diff_l + diff_u).mean(axis=1)
        \begin{equation*}
            L_{m} = \frac{1}{\abs{\mathcal{I}_{\text{test}}}}\sum_{i = \mathcal{I}_{\text{test}}} \pqty{\max\cqty{y_{i}^{l} - \hat{f}_{m}(x_{i}), 0}^{2} + \max\cqty{\hat{f}_{m}(x_{i}) - y_{i}^{u}, 0}^{2}}.
        \end{equation*}
        and then find the  minimum \(L^{*} = \min_{m} L_{m}\) and \(\mathcal{M}^{*} = \cqty{m: L_{m} \leq L^{*} + \tau}\), where \blue{\(\tau\)} is the \blue{tolerance}. And the \textit{post-selection prediction sets} are \(\hat{Y}(x) = \cqty{\hat{f}_{m}(x): m\in \mathcal{M}^{*}}\).
    \end{enumerate}

    \section{Simulations about the methods of regression}
    


    \section{Simulations about the random draws \tmath{y_{d}}}

    There are two considerations about the ways we draw \(y_{d}\). Firstly, we need to decide on the number of draws. Secondly, we need to decide on the distribution of the draws. 

    \subsection{Number of draws seem to not matter much}


% The End
% \printbibliography
\end{document}
\documentclass[12pt]{article}
\usepackage{myart}\addbibresource{/Users/lwg342/Documents/LaTeX/lib.bib}
\newcommand{\cqty}[1]{\left\{ #1 \right\}}
%--------Hello World--------%
%=====================================%
\begin{document}
    \title{Prediction with interval outcomes}\author{}\date{}\maketitle

    \section{Simulation 2023-11-29}
    \blue{Blue items are parameters of simulation designs that can be changed.}

    We randomly generate \blue{sample size \(n\)} observations of \((y,x)\), where \(x\) is \blue{\(k\)-dimensional},
    \begin{equation*}
        y_{i} = f(x_{i}) + \epsilon_{i}.
    \end{equation*}
    where \(\epsilon\) are independent draws from from either a normal distribution with variance \blue{\(v_{\epsilon}\)} or a centered \(\chi^{2}\) distribution with \blue{\(\text{df}\)} degrees of freedom \(\epsilon_{i} \sim \frac{\chi^{2}_{\text{df}} - \text{df}}{\sqrt{2 \text{df}}}\). 
    Let's assume we observe not \(y_{i}\), but \(y^{l}_{i}\) and \(y_{i}^{u}\). In the simulation, it's generated by 
    \begin{align*}
        y_{i}^{l} &= \lfloor y_{i} \rfloor - a_{1} \\
        y_{i}^{u} &= \lceil y_{i} \rceil  + a_{2},
    \end{align*}
    where \blue{\(a_{1},a_{2}\)} will control how uncertain we are about our observations of \(y\), in what follows let's first take \(a_{1} = a_{2} = 0\). Let all \(\beta\)'s be \(1\), and \(x_{i}\) either have \blue{normal \(N(0,I_{k})\) or uniform distribution \(U( - \sqrt{3},\sqrt{3})\)}. We follow Elie's suggestions and do the following:
    \begin{enumerate}
        \item Split the sample into training and testing sets. 
        \item Randomly pick \(\lambda^{m} \sim \blue{U(0,1)}\),  \(y^{m}_{i} = \lambda^{m}y^{l}_{i} + (1 - \lambda^{m}) y^{u}_{i}\), \(m = 1,\dots, \blue{M}\), and use them as the training set. Fit \(M\) regression models \(\hat{f}_{m}\). In the simulations, we consider \blue{linear regression} or \blue{local linear regression}. Let \(\tilde{Y}(x) = \left\{\hat{f}_{m}(x): 1\leq m\leq M\right\}\) be the \textit{pre-selection prediction sets} for a given evaluation point \(x\), which is the set of predictions from all models.
        \item Compute the score \(S_{m}\) for each model on the testing set, find the  minimum \(S_{m}^{*}\) and \(\mathcal{M}^{*} = \cqty{m: S_{m} \leq S_{m}^{*} + \tau}\), where \blue{\(\tau = \frac{1}{\sqrt{n}}\)} is the \blue{tolerance}. And the \textit{post-selection prediction sets} are \(\hat{Y}(x) = \cqty{\hat{f}_{m}(x): m\in \mathcal{M}^{*}}\).
    \end{enumerate}
    The following are simulation results plotted, where we fix \(x_{1} = x_{2} = x_{3} = 0\) and plot the pre- and post-selection prediction sets \(\tilde{Y}, \hat{Y}\) against varying \(x_{4}\), with the parameters of design shown on the top of each plot. We fix sample size \(n = 1000\), number of draws \(M = 100\), and number of features \(k = 4\).  

    \subsection{Linear Model}

    In this part, the true model \(y_{i} = \sqrt{2} + x_{i}'\beta + \epsilon_{i}\), where \(\beta = \frac{\pi}{10} \mathbf{1}_{k}\). We will call \(y_{i} - \epsilon_{i}\) the signal which is the blue line in the following plots. Green lines show the interval surrounding the signal. And the purple line ``fit y\_middle'' is the prediction we will make if we train the model using the middle points \(\frac{1}{2}(y_{u,i} + y_{l,i})\).
    
    Figure \ref{fig:1} uses OLS estimators and Figure \ref{fig:2} uses local linear regression when \(x\sim N(0,I_{k})\) and \(\epsilon\sim N(0,1)\). If we let the distribution of \(x\)'s be uniform \(U( - \sqrt{3}, \sqrt{3})\) and \(\epsilon\) be centered \(\chi^{2}\), we get Figures \ref{fig:3} and \ref{fig:4}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291105-linear-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are linear regressions.}
        \label{fig:1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291105-loclin-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are local linear regressions.}
        \label{fig:2}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291109-linear-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are linear regressions.}
        \label{fig:3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291109-loclin-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are local linear regressions.}
        \label{fig:4}
    \end{subfigure}
    \caption{Comparison of linear and local linear regressions.}
    \label{fig:comparison1}
\end{figure}
\newpage 


\subsection{Quadratic Model}

If the true model is 
\begin{equation*}
    y_{i} = \sqrt{2} + x_{i}'\beta + \frac{1}{5} x_{i,4}^{2}
\end{equation*}
the results are in Figure \ref{fig:comparison2}.



\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291113-linear-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are linear regressions.}
        \label{fig:5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291113-loclin-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are local linear regressions.}
        \label{fig:6}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291115-linear-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are linear regressions.}
        \label{fig:7}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291115-loclin-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are local linear regressions.}
        \label{fig:8}
    \end{subfigure}
    \caption{Comparison of linear and local linear regressions for quadratic models.}
    \label{fig:comparison2}
\end{figure}
\newpage 


\subsection{Cosine models}
If the true model is 
\begin{equation*}
    y_{i} = \sqrt{2} + x_{i}'\beta + \cos(x_{i,4})
\end{equation*}
the results are in Figure \ref{fig:comparison3}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291118-linear-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are linear regressions.}
        \label{fig:9}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291118-loclin-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are local linear regressions.}
        \label{fig:10}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291120-linear-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are linear regressions.}
        \label{fig:11}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../simulation-results/202311291120-loclin-1000-100-4.pdf}
        \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are local linear regressions.}
        \label{fig:12}
    \end{subfigure}
    \caption{Comparison of linear and local linear regressions for quadratic models.}
    \label{fig:comparison3}
\end{figure}
\newpage

\section{Comments}

\begin{enumerate}
    \item Another way to draw \(y_{i}^{m}\) for \(m = 1,\cdots,M\)  is by drawing from \(U(y_{l,i}, y_{ui})\) as compared with the current way of drawing \(\lambda^{m}\) first (which is fixed for all \(i\)). But it seems to be very close to using the middle point, because \(y_{i}^{m} = \frac{1}{2} \pqty{y_{l,i} + y_{u,i}} + u\), where \(u\) is some uniform noise. Below is a graph  when the true model is linear. The prediction sets are much narrower. 
    \begin{figure}[h]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth]{../simulation-results/202311291134-linear-1000-100-4.pdf}
            \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are linear regressions.}
            \label{fig:13}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}
            \includegraphics[width=\textwidth]{../simulation-results/202311291134-loclin-1000-100-4.pdf}
            \caption{We fix \(x_{1} = x_{2} = x_{3} = 0\), while \(x_{4}\) varies. \(\hat{f}\) are local linear regressions.}
            \label{fig:14}
        \end{subfigure}
    \caption{Using random draws from \(U(y_{l,i}, y_{u,i})\) instead of \(\lambda^{m}y_{l,i} + (1 - \lambda^{m}) y_{u,i}\).}
    \label{fig:comparison4}
    \end{figure}
    \item Let's visualize the difference between these two ways of drawing, by considering \(y = x\) and \(y_{l} = \lfloor x  \rfloor\) and \(y_{u} = \lceil x \rceil\). The following plots two typical drawed samples \(y^{m_{1}}_{i}, y^{m_{2}}_{i}\) using the two methods.
    \begin{figure}[htbp]
        \centering
        \includegraphics{../simulation-results/202311291211-sample-uniform.pdf}
        \caption{Uniform draws from \((y_{l}, y_{u})\).}
    \end{figure}

    \begin{figure}[htbp]
        \centering
        \includegraphics{../simulation-results/202311291211-sample-fixed-weight.pdf}
        \caption{Linear combination of \(y_{l}\) and \(y_{u}\) with fixed weights.}
    \end{figure}

\end{enumerate}



% The End
% \printbibliography
\end{document}